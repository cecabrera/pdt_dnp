---
title: "Clasificación de Planes de Desarrollo"
author: "Camilo Cabrera"
date: "December 21, 2017"
runtime: shiny
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Los Planes de Desarrollo Territorial (PDT) son los documentos que dan la hoja de ruta de cada Departamento de Colombia durante los 4 años de gobierno electo. Los documentos con sus contenidos textuales están disponibles [aquí](/data.zip). Putumayo es el único [plan de desarrollo](https://www.putumayo.gov.co/images/documentos/planes_y_programas/ordeN_726_16.pdf) faltante en esta base de datos.

Se analizarán los siguientes puntos:

1. Análisis de temas principales en cada PDT.
2. Clasificar los planes de acuerdo con algún criterio de similitud.
3. ¿Qué tipo de información reportaría a los evaluadores con el fin de hacer su tarea más
sencilla?

Analizar tal cantidad de datos (31 documentos y 59004 palabras) simultáneamente es una labor posible gracias a la [minería de texto](https://es.wikipedia.org/wiki/Miner%C3%ADa_de_textos). El objetivo de este blog es organizar, clasificar y visualizar los datos utilizando el lenguaje de programación [R](https://www.r-project.org/). El código fuente de este documento está alojado en [Github](https://github.com/cecabrera/pdt_dnp).

Los paquetes de R necesarios para este análisis serán:

```{r}
require(shiny)
require(data.table)
require(tm)
require(ggplot2)
require(corrplot)
require(tidyr)
require(ggplot2)
require(dplyr)
require(tidytext)
require(topicmodels)
```

Para comenzar, se almacena las rutas de los archivos de texto en una variable llamada `paths`. Esta lista será la ruta para cargar cada Plan de Desarrollo. Los nombres de cada Departamento se almacenarán en la variable `departaments`. 

```{r}
paths <- list.files(c("data"), pattern="\\.(TXT|txt)$", recursive = TRUE, full.names = TRUE)
departments <- sub(pattern = ".txt.*", "" , sub(pattern = ".*/", "", paths))
head(paths, 5) # Visualizar los primeros 5 elementos de la variable `paths`
```

# Preprocesamiento

El preprocesamiento es un componente clave en muchos algoritmos de minado de texto y usualmente se resume en tareas como tokenización, filtrado, lematización y _stemming_ (o raíz de las palabras).

Se cargan los 31 documentos datos en una sola lista llamada `d`. Cada elemento de la lista representará un Departamento y tendrá las palabras de su respectivo PDT. Se adiciona a cada palabra un `1` para denotar que la palabra aparece una vez. Este procedimiento ayudará a agrupar las frecuencias de palabras.

```{r read, eval=FALSE}
d <- lapply(paths, function(x){
  bag = data.table(names(fread(input = x, encoding = "UTF-8")))
  setnames(x = bag, old = "V1", new = sub(pattern = ".txt.*", "" , sub(pattern = ".*/", "", x)))
  bag[, f := 1]
  bag
})
names(d) <- departments
```

## Tokenización

La tokenización consiste en separar las secuencias de caracteres en pedazos (palabras/frases) llamados tokens y, en el camino, desechar caracteres como puntuaciones. Nuestros documentos ya carecen de puntuaciones y están separados en su mayoría por palabras. 

## Filtrado

Un filtro común es remover las _stop words_ o palabras que frecuentemente aparecen en el texto y que no aportan información: preposiciones y conjunciones, por citar algunas. En R, el paquete `tm` brinda una lista de 308 `stop-words` del español. Así mismo, hay palabras que aparecen con mucha frecuencia y que son irrelevantes como "ee", "nn", "oo", "aa", etc. También serán filtradas. 

 A continuación las primeras 50. 

```{r}
stop_es <- c(stopwords(kind = "es"), "ee", "nn", "oo", "aa", "dd", "rr", "cc", "pp", "tt", "yy", "zz", "xx")
print(matrix(stop_es[1:50], nrow = 10, byrow = TRUE))
```

Las suprimimos de todos los documentos:

```{r filter, eval=FALSE}
d <- lapply(d, function(x){
  x[!x[[1]] %in% stop_es]
})
```

## Lematización

Lematización considera el análisis morfológico de las palabras: agrupar las diversas formas de una palabra de manera que puedan ser analizadas como un solo término. En otras palabras, los métodos de lematización intentan mapear verbos a su sentido infinitivo y los sustantivos a su forma singular. Para lematizar documentos, es necesario determinar para cada palabra si es un verbo, adjetivo o sustantivo. Dado que este proceso es tedioso y sujeto a errores, en la práctica, se prefieren los métodos de _stemming_. 

## Stemming

Los métodos de _stemming_ le apuntan a encontrar la raíz de las palabras y usarla como método agrupador de palabras. Como es de esperarse, los algoritmos de _stemming_ dependen del lenguaje. 

Utilizando el algoritmo de _stemming_ de Porter y tomando como ejemplo las 30 palabras más mencionadas del Plan de Desarrollo del Huila encontramos las siguientes comparaciones. 

```{r}
d <- readRDS("chunks/filter.RDS")
print(d$HUILA[, .N, keyby = HUILA][order(N, decreasing = T)][1:30,.(HUILA, stemDocument(x = HUILA,  language = "es"), N)])
```

La búsqueda del origen de las palabras _(stemming)_ es un procedimiento delicado. En el español, remover las tildes en las palabras cambia el acento y puede entregar una connotación distinta a la palabra de origen: no es lo mismo "hábito" que "habito" que "habitó". A su vez, las raíces agrupan palabras de contexto diferentes: "partido", "parte", "partida" y "partes" conllevan a la misma raíz "part". Para efectos de este trabajo se utilizarán las raíces de las palabras dejando a consideración del lector las implicaciones que esto tiene. 

```{r stemm, eval=FALSE}
d <- lapply(d, function(x){
  colname <- colnames(x)[1]
  x[[colname]] <- stemDocument(x[[colname]], language = "es")
  x
})
```

# Modelo de Vectores Espaciales

Se definirá el _vocabulario_ como el conjunto que contiene todas las palabras presentes en todos los documentos. La manera más común de representar a todos los documentos es convirtiéndolos en vectores numéricos. A esta representación se le llama "Modelo de Vectores Espaciales" (VSM) y es utilizada en algoritmos de minado de texto y sistemas de recolección de información; al tiempo que facilita análisis eficientes de grandes colecciones de documentos. 

Colocamos todos los tokens en una misma matriz, se suman las frecuencias de cada palabra en cada documento y se ordenan en orden decreciente.  

```{r frequencies}
d <- readRDS("chunks/stemm.RDS")
d <- lapply(d, function(x){
  name <- colnames(x)[1]
  x[, depart := name]
  setnames(x, old = name, new = "word")
  x
})
d <- rbindlist(d, use.names = T)[, sum(f), keyby = .(depart, word)]
print(d)
```

Filtra por los departamentos. Cada departamento resalta las 15 palabras (raíces) más mencionadas en cada Plan de Desarrollo. 

```{r echo=FALSE}
wellPanel(
  fluidRow(
    column(4
      , selectInput("depart1", label = "Selecciona el departamento:",
      choices = departments, selected = "CASANARE", multiple = F)
      
    , renderPlot(expr ={
        ggplot(data = d %>% filter(depart %in% input$depart1) %>% mutate(word = reorder(word, V1)) %>% top_n(15, V1), aes(word, V1, fill = depart)) +
          geom_col(show.legend = FALSE) + labs(x = NULL, y = "frecuencia") +
          # facet_wrap(~depart, ncol = 2, scales = "free") +
          coord_flip() + scale_fill_manual(values=c("#CC6666", "#9999CC", "#66CC99"))
    })

    )
    , column(4
    , selectInput("depart2", label = "Selecciona el departamento:",
      choices = departments, selected = "CUNDINAMARCA", multiple = F)
    , renderPlot(
        ggplot(data = d %>% filter(depart %in% input$depart2) %>% mutate(word = reorder(word, V1)) %>% top_n(15, V1), aes(word, V1, fill = depart)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "frecuencia") +
        # facet_wrap(~depart, ncol = 2, scales = "free") +
        coord_flip()+
        scale_fill_manual(values=c("#9999CC"))
    )
    )
    , column(4
    , selectInput("depart3", label = "Selecciona el departamento:",
    choices = departments, selected = "ANTIOQUIA", multiple = F)
    , renderPlot(
        ggplot(data = d %>% filter(depart %in% input$depart3) %>% mutate(word = reorder(word, V1)) %>% top_n(15, V1), aes(word, V1, fill = depart)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "frecuencia") +
        # facet_wrap(~depart, ncol = 2, scales = "free") +
        coord_flip() + 
        scale_fill_manual(values=c("#66CC99"))
    )

    )
  )
)
```

En los documentos está el reto de lidiar con las palabras pegadas. Por ejemplo, en Casanare hay palabras como "culturaprogram", "comunitarioprogram", "deportivosector" y "firmesector". En Arauca sucede algo similar: "propiosotr" haciendo alución a "propios otros" y "físicasecret" haciendo alución a "física secreto". Estas inconsistencias podrían dificultar la comparación entre documentos. 

En el Meta está de primero la palabra "guaro". Indagando en el documento se encontró que hay un municipio llamado "San Carlos de Guaroa" y el algoritmo luego de hacer el _stemm_ suprimió la "a" al final de "Guaroa". Este municipio tiene relevancia por las intenciones del departamento en aportar a sus necesidades sociales y económicas; y no tiene nada que ver con una política hacia el aguardiente. 

En VSM cada palabra es representada por un valor numérico que indica el peso (importancia) de la palabra en el documento. Se utilizará el modelo de _Term frequency-inverse document frequency_ (TF-IDF) para ponderar. Se moldea la tabla en una matriz de frecuencia:

```{r}
f <- dcast.data.table(d, formula = "word ~ depart", value.var = "V1", fill = 0)
str(f)
```

La variable `f` contiene la frecuencia con la que aparece cada palabra para cada departamento. Se calcula el término de frecuencia `TF-IDF`:

```{r weights, eval=FALSE}
tf <- f[,-1,with=FALSE]
idf <- log((ncol(f)-1)/rowSums(tf))
q <- tf*idf
str(q)
q_melted <- melt.data.table(cbind(word = f$word, q), id.vars = "word", variable.name = "depart", value.name = "V1")
```

```{r echo=FALSE}
q <- readRDS("chunks/weights_q.RDS")
q_melted <- readRDS("chunks/weights_q_melted.RDS")
```

La variable `q` contiene la frecuencia ponderada por el TF-IDF. La variable `q_melted` es la misma variable `q` reducida en 3 columnas. A continuación las palabras más relevantes de cada departamento luego de ponderar por TF-IDF.

```{r echo=FALSE}
wellPanel(
  fluidRow(
    column(4
      , selectInput("depart4", label = "Selecciona el departamento:",
      choices = departments, selected = "CASANARE", multiple = F)
      
    , renderPlot(
        ggplot(data = q_melted %>% filter(depart %in% input$depart4) %>% mutate(word = reorder(word, V1)) %>% top_n(15, V1), aes(word, V1, fill = depart)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "TF-IDF") +
        # facet_wrap(~depart, ncol = 2, scales = "free") +
        coord_flip() + 
        scale_fill_manual(values=c("#CC6666"))
    )

    )
    , column(4
    , selectInput("depart5", label = "Selecciona el departamento:",
      choices = departments, selected = "CUNDINAMARCA", multiple = F)
    , renderPlot(
        ggplot(data = q_melted %>% filter(depart %in% input$depart5) %>% mutate(word = reorder(word, V1)) %>% top_n(15, V1), aes(word, V1, fill = depart)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "TF-IDF") +
        # facet_wrap(~depart, ncol = 2, scales = "free") +
        coord_flip() + 
        scale_fill_manual(values=c("#9999CC"))
    )
    )
    , column(4
    , selectInput("depart6", label = "Selecciona el departamento:",
    choices = departments, selected = "ANTIOQUIA", multiple = F)
    , renderPlot(
        ggplot(data = q_melted %>% filter(depart %in% input$depart6) %>% mutate(word = reorder(word, V1)) %>% top_n(15, V1), aes(word, V1, fill = depart)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = "TF-IDF") +
        # facet_wrap(~depart, ncol = 2, scales = "free") +
        coord_flip() + 
        scale_fill_manual(values=c("#66CC99"))
    )

    )
  )
)


```

Las [4 métricas](https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words) de similaridad más utilidades en la literatura son:

* Jaccard Similarity
* Cosine Similarity
* Spearman's rank correlation
* Pearson chi2 test-based

Según [Kilgarriff (1997)](http://www.aclweb.org/anthology/W/W97/W97-0122.pdf), el test ideal para calcular la similaridad entre dos documentos es el _Pearson chi2 test-based_. Sin embargo, el poder computacional y concepto teórico es significativamente superior a utilizar _Cosine Similarity_ y entregan resultados relativamente similares. El _Cosine Similarity_ ha demostrado en otros experimentos ser mejor que el _Spearman's rank correlation_; aún cuando es una alternativa viable para calcular la similaridad entre documentos. La _Jaccard Similarity_ es la más simple de las similaridades y no tiene en consideración la frecuencia con la que aparecen las palabras. Por esta razón, la similaridad a trabajar será el _Cosine Similarity_.

Cada Plan de Desarrollo se puede representar como un __vector__ de valores. El coseno entre dos vectores determina cuán cercanos (igual a 1) o alejados (igual a -1) están en el plano n-dimensional. El cálculo del coseno es considerado también como el [_coeficiente de correlación no centrado_](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) entre dos vectores y matemáticamente es similar al coeficiente de correlación de Pearson. Por eso se usará este como medida de la similaridad. 

```{r}
corrplot(corr = cor(q, method = "pearson"), type = "lower", method = "pie", diag = TRUE, order = "FPC")
```

La matriz de correlación se ordenó por compomentes principales y por eso aparecen primero los departamentos con mayor correlación y de último los de menos. Se resalta los siguientes insights:

* Quindío y Cundinamarca tienen la mayor similitud (0.903) de palabras en los Planes de Desarrollo. 
* Casanare es el departamento con mayor diferencias respecto a los demás departamentos.
* [En Octubre de 2016](http://www.portafolio.co/economia/gobierno/dnp-destaco-los-planes-de-desarrollo-departamentales-501057), el DNP entregó un reconocimiento a las gobernaciones con desarrollo robusto, intermedio y temprano a Antioquia, Nariño y Caquetá, respectivamente. En el correlograma, estas tres aparecen seguidas infiriendo esta aparente similitud. 

# Clasificación

Si cada documento hiciese parte de una categoría _a priori_, se utilizarían algorirmos como:

1. Clasificadores del vecino más cercano
2. Clasificadores de árboles de decisión
3. Máquinas de Vectores de Soporte
4. Clasificador Naive Bayes

Un caso podría ser categorizar los documentos por región del país, tomar los Planes de Desarrollo Municipales y tratar de clasificar si las palabras usadas en los Municipios están correlacionadas (son similares) a las palabras usandas en los Departamentos. 

En vista que la clasificación se hará _a posteriori_, se utilizarán algoritmos de _clustering_ o conglomerados. En Analítica de Texto algunos son:

1. Algortimos de Conglomerados Herárquicos. 
2. Conglomerado k-medias
3. Conglomerados probabilísticos y Modelos de Tópicos

Los algoritmos de conglomerados herárquicos están basados en distancias. Utilizan una función de similaridad para medir cuán cerca está un documento de otro. Es posible contruirlos de arriba-a-abajo (colocando cada documento en un grupo) o de abajo-a-arriba (colocando todos los documentos en un grupo). Los algoritmos de k-medias ubican un centroide por cada grupo y conglomera cada documento respecto a cada centroide. Es ampliamente utilizado en problemas de datos estructurados. El cálculo del número óptimo de grupos (k) es considerado _NP-Hard_ dados los requerimientos computacionales para calcularlo. 

La mayor implicación en los algoritmos anteriores es que a cada "observación" (en nuestro caso "palabra") es asignado a una única categoría. Sin embargo, existen palabras que podrían hacer parte de varias categorías (o "tópicos"). Por eso, el modelaje de tópicos será el escogido para clasificar los Planes de Desarrollo Departamentales.

# Modelaje de tópicos

Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language.

## Asignación de Dirichlet Latente

LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document. There are a number of existing implementations of this algorithm, and we’ll explore one of them in depth.

```{r}
doc <- cast_dtm(data = d, document = depart, term = word, value = V1, weighting = tm::weightTf)
print(doc)
```

Se analizarán 2 tópicos

```{r}
ap_lda2 <- LDA(doc, k = 2, control = list(seed = 1234))
print(ap_lda2)
```

Se extrae las probabilidades o β (“betas”) del modelo por tópico por palabra.

```{r}
ap_topics2 <- tidy(ap_lda2, matrix = "beta")
print(ap_topics2)
```

Notice that this has turned the model into a one-topic-per-term-per-row format. For each combination, the model computes the probability of that term being generated from that topic. For example, the term “aaron” has a  
1.686917×10−12 1.686917×10−12  probability of being generated from topic 1, but a  3.8959408×10−
53.8959408×10−5 probability of being generated from topic 2.

```{r}
ap_top_terms2 <- ap_topics2 %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms2 %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```


This visualization lets us understand the two topics that were extracted from the articles. The most common words in topic 1 include “percent”, “million”, “billion”, and “company”, which suggests it may represent business or financial news. Those most common in topic 2 include “president”, “government”, and “soviet”, suggeting that this topic represents political news. One important observation about the words in each topic is that some words, such as “new” and “people”, are common within both topics. This is an advantage of topic modeling as opposed to “hard clustering” methods: topics used in natural language could have some overlap in terms of words.

As an alternative, we could consider the terms that had the greatest difference in  
β
β  between topic 1 and topic 2. This can be estimated based on the log ratio of the two:  
log
2
(
β
2
β
1
)
log2a(β2β1)  (a log ratio is useful because it makes the difference symmetrical:  
β
2
β2  being twice as large leads to a log ratio of 1, while  
β
1
β1  being twice as large results in -1). To constrain it to a set of especially relevant words, we can filter for relatively common words, such as those that have a β  greater than 1/1000 in at least one topic.

```{r}
beta_spread2 <- ap_topics2 %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread2 %>%
  mutate(term = reorder(term, log_ratio)) %>%
  filter(abs(log_ratio) > 4) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col(show.legend = FALSE) + 
  coord_flip()
```

# Document-topic probabilities

```{r}
ap_documents <- tidy(ap_lda2, matrix = "gamma")
print(ap_documents)

# plot_ly(data = {
#   
#           temp <- as.data.table(ap_documents)[topic %in% "1"][order(gamma)]
#           
# }
#         , x = ~document, y = ~gamma, type = "bar")
```

# Recomendaciones

1. El hecho que todos los PDT hayan tenido una correlación relativamente alta entre si (superior al 60%), enciende las alarmas de por qué Casanare es el más alejado. Este Departamento rico en petróleo atraviesa [crisis sociales](http://www.colombiainforma.info/la-crisis-que-deja-el-petroleo-en-casanare/) y [escándalos de corrupción](https://www.youtube.com/watch?v=UibMVMbO9Jk). Una hipótesis posible sería investigar si este Plan de Desarrollo contiene lineamientos que expliquen la corrupción.
2. 

# Referencias

* [A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques](https://arxiv.org/pdf/1707.02919.pdf)
* [Text Mining with R](https://www.tidytextmining.com/tfidf.html)
* [Quantify the similarity of bags of words](https://stats.stackexchange.com/questions/289400/quantify-the-similarity-of-bags-of-words)
* [Pearson Correlation Coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)
* [DNP destacó los planes de desarrollo departamentales](http://www.portafolio.co/economia/gobierno/dnp-destaco-los-planes-de-desarrollo-departamentales-501057) 
* [Text Mining with R](https://www.tidytextmining.com/topicmodeling.html)


